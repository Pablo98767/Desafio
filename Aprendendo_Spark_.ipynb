{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMg25nj8NAt/dIIexN8JEJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pablo98767/Desafio/blob/main/Aprendendo_Spark_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fazendo a instalação do spark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "DtE_c8mNbdCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importação da biblioteca \n",
        "import pyspark"
      ],
      "metadata": {
        "id": "7oKxiIh3biCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando a biblioteca e iniciando uma sessão spark para iniciar a aplição.\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "tRk_PPl0cWN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Criando um Dataframe\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Or2p1Muxc9Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importando bibliotecas do spark para trabalhar com criação de dataframe e algumas auxiliares\n",
        "from datetime import datetime, date\n",
        "import pandas as pd\n",
        "from pyspark.sql import Row\n"
      ],
      "metadata": {
        "id": "rKUSipUAcv9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Em primeiro lugar, você pode criar um PySpark DataFrame a partir de uma lista de linhas\n",
        "df = spark.createDataFrame([\n",
        "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
        "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
        "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
        "])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "_Vzx2-nfdIvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie um DataFrame PySpark a partir de um DataFrame pandas\n",
        "\n",
        "pandas_df = pd.DataFrame({\n",
        "    'a': [1, 2, 3],\n",
        "    'b': [2., 3., 4.],\n",
        "    'c': ['string1', 'string2', 'string3'],\n",
        "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
        "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
        "})\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "df"
      ],
      "metadata": {
        "id": "ohkr1kdHdfQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie um PySpark DataFrame com um esquema explícito.\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
        "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
        "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
        "], schema='a long, b double, c string, d date, e timestamp')\n",
        "df"
      ],
      "metadata": {
        "id": "5a8XdVtGdu8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***OBS : os dataframes a cima trazem os mesmo dados, só são 3 tipos de maneiras que você pode usar para criar dataframes.*** "
      ],
      "metadata": {
        "id": "sEdms3QMeLAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando os dados\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "gbV22MVKejV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizando os Dados"
      ],
      "metadata": {
        "id": "phnCpi1EexQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(1)"
      ],
      "metadata": {
        "id": "_KGVYMg4e2UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBS : Como alternativa, você pode habilitar spark.sql.repl.eagerEval.enableda configuração para a avaliação antecipada do PySpark DataFrame em notebooks como o Jupyter. O número de linhas a serem exibidas pode ser controlado por meio spark.sql.repl.eagerEval.maxNumRowsda configuração."
      ],
      "metadata": {
        "id": "NuMM8caWfDUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# O comando abaixo chama o spark.sql.repl.eagerEval.enabled. Em seguida nós chamamos nosso df\n",
        "# O objetivo é trazer uma tabela melhorada para nosso dataframe, facilitando nosso visual.\n",
        "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
        "df"
      ],
      "metadata": {
        "id": "5S1piObVfFxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#As linhas também podem ser mostradas verticalmente. Isso é útil quando as linhas são muito longas para serem exibidas horizontalmente\n",
        "df.show(1, vertical=True)"
      ],
      "metadata": {
        "id": "QodcoI-QfjR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Podemos  ver o esquema do DataFrame e os nomes das colunas da seguinte maneira:***"
      ],
      "metadata": {
        "id": "Fg6XaM6CftRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando o comando do pandas\n",
        "df.columns"
      ],
      "metadata": {
        "id": "tnLO8ABsf4Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usando o print do spark \n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "czV9VzCJf-LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mostrar o resumo do DataFrame usando a função select\n",
        "df.select(\"a\", \"b\", \"c\").describe().show()"
      ],
      "metadata": {
        "id": "8iwluxz3gEyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A função abaixo coleta os dados distribuídos para o lado do driver como os dados locais em Python. \n",
        "# Observe que isso pode gerar um erro de falta de memória quando o conjunto de dados é muito grande para caber no lado do driver porque ele coleta todos os dados dos executores para o lado do driver.\n",
        "df.collect()"
      ],
      "metadata": {
        "id": "hZCooe-AgR1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para evitar lançar uma exceção de falta de memória, use DataFrame.take()ou DataFrame.tail().\n",
        "df.take(1)"
      ],
      "metadata": {
        "id": "0lt9pgBjgiqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#O PySpark DataFrame também fornece a conversão de volta para um DataFrame do pandas para aproveitar a API do pandas. \n",
        "#Observe que toPandas também coleta todos os dados no lado do motorista que podem facilmente causar um erro de falta de memória \n",
        "#quando os dados são muito grandes para caber no lado do motorista.\n",
        "df.toPandas()"
      ],
      "metadata": {
        "id": "gHRAWbjPgxt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Selecionando e acessando dados"
      ],
      "metadata": {
        "id": "x0ZcapDvhBq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***O PySpark DataFrame é avaliado lentamente e simplesmente selecionar uma coluna não aciona o cálculo, mas retorna uma Columninstância.***"
      ],
      "metadata": {
        "id": "VR99jTEkhKoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#selecionando a coluna a com pandas\n",
        "df.a"
      ],
      "metadata": {
        "id": "0MwEzQfzhQSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Na verdade, a maioria das operações em colunas retornam Columns.***"
      ],
      "metadata": {
        "id": "kx1bPCjEhaX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#usando o sistema de seleção de coluna do spark\n",
        "from pyspark.sql import Column\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "type(df.c) == type(upper(df.c)) == type(df.c.isNull())"
      ],
      "metadata": {
        "id": "EQTtoymWhdrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Esses Columns podem ser usados ​​para selecionar as colunas de um DataFrame. Por exemplo, DataFrame.select()pega as Columninstâncias que retornam outro DataFrame.***"
      ],
      "metadata": {
        "id": "1bcpfNp5h11T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df.c).show()"
      ],
      "metadata": {
        "id": "73p3OjGch5r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Atribuir nova Columninstância.\n",
        "\n",
        "df.withColumn('upper_c', upper(df.c)).show()\n",
        "# neste caso foi criada uma coluna chamada upper_c, e depois ela recebeu os dados da coluna c usando o comando upper_c."
      ],
      "metadata": {
        "id": "IPwV7HsziD1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para selecionar um subconjunto de linhas, use DataFrame.filter().\n",
        "df.filter(df.a == 1).show()"
      ],
      "metadata": {
        "id": "wbajWhjmiehd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aplicando uma função"
      ],
      "metadata": {
        "id": "oyJnf0v8iyNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***O PySpark oferece suporte a vários UDFs e APIs para permitir que os usuários executem funções nativas do Python. Consulte também as últimas UDFs do Pandas e APIs de função do Pandas . Por exemplo, o exemplo abaixo permite que os usuários usem diretamente as APIs em uma série de pandas dentro da função nativa do Python.***"
      ],
      "metadata": {
        "id": "ofKI_CQyi3O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as bibliotecas para criar nossas funções\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf"
      ],
      "metadata": {
        "id": "sU3dYynBi7if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#criando as funções\n",
        "\n",
        "@pandas_udf('long')\n",
        "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
        "    # Simply plus one by using pandas Series.\n",
        "    return series + 1\n",
        "\n",
        "df.select(pandas_plus_one(df.a)).show()"
      ],
      "metadata": {
        "id": "XMD-74-jjD9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Outro exemplo é DataFrame.mapInPandasque permite que os usuários usem diretamente as APIs em um DataFrame pandas sem quaisquer restrições, como o comprimento do resultado.***"
      ],
      "metadata": {
        "id": "wBo6gjF-j9aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pandas_filter_func(iterator):\n",
        "    for pandas_df in iterator:\n",
        "        yield pandas_df[pandas_df.a == 1]\n",
        "\n",
        "df.mapInPandas(pandas_filter_func, schema=df.schema).show()"
      ],
      "metadata": {
        "id": "BgxjoAthkAtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dados de agrupamento"
      ],
      "metadata": {
        "id": "xNuQtJgCmTls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** PySpark DataFrame também fornece uma maneira de lidar com dados agrupados usando a abordagem comum, estratégia de divisão-aplicação-combinação. Ele agrupa os dados por uma determinada condição, aplica uma função a cada grupo e os combina de volta ao DataFrame.***"
      ],
      "metadata": {
        "id": "95WuPShPmnQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([\n",
        "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
        "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
        "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
        "df.show()"
      ],
      "metadata": {
        "id": "rlUi537emti5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Agrupar e depois aplicar a avg()função aos grupos resultantes.\n",
        "df.groupby('color').avg().show()"
      ],
      "metadata": {
        "id": "ikJPFINHmyKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Você também pode aplicar uma função nativa do Python em cada grupo usando a API do pandas.\n",
        "def plus_mean(pandas_df):\n",
        "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
        "\n",
        "df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()"
      ],
      "metadata": {
        "id": "0_TXeGXQoKC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Co-agrupamento e aplicação de uma função.\n",
        "df1 = spark.createDataFrame(\n",
        "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
        "    ('time', 'id', 'v1'))\n",
        "\n",
        "df2 = spark.createDataFrame(\n",
        "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
        "    ('time', 'id', 'v2'))\n",
        "\n",
        "def asof_join(l, r):\n",
        "    return pd.merge_asof(l, r, on='time', by='id')\n",
        "\n",
        "df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n",
        "    asof_join, schema='time int, id int, v1 double, v2 string').show()"
      ],
      "metadata": {
        "id": "kBarjjaGoa5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Obtendo dados dentro/fora ¶"
      ],
      "metadata": {
        "id": "gFab00qOorDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***O CSV é direto e fácil de usar. Parquet e ORC são formatos de arquivo compactos e eficientes para leitura e gravação mais rápidas.\n",
        "\n",
        "Existem muitas outras fontes de dados disponíveis no PySpark, como JDBC, texto, binaryFile, Avro, etc. Consulte também o guia mais recente do Spark SQL, DataFrames e conjuntos de dados na documentação do Apache Spark.\n",
        "***"
      ],
      "metadata": {
        "id": "uTgwUp40o5ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Leitura CSV"
      ],
      "metadata": {
        "id": "f-HL2nSHqLW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cria  arquivos csv do nosso dataframe \n",
        "df.write.csv('/content/foo.csv', header=True)\n",
        "\n",
        "# aplica a leitura dos dados csv\n",
        "spark.read.csv('/content/foo.csv', header=True).show()"
      ],
      "metadata": {
        "id": "T8zxbzfxpIKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-E7qUiGfqNtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Leitura Parquet"
      ],
      "metadata": {
        "id": "BOU2070mp61J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pega o dataframe e tranforma em arquivo parquet\n",
        "df.write.parquet('bar.parquet')\n",
        "#faz a leitura do arquivo parquet\n",
        "spark.read.parquet('bar.parquet').show()"
      ],
      "metadata": {
        "id": "1hg5JJZwp9x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Leitura de arquivos orc"
      ],
      "metadata": {
        "id": "PYZP3QUjqUmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Trabsforma nosso dataframe em arquivo orc\n",
        "df.write.orc('zoo.orc')\n",
        "\n",
        "#fa a leitura do arquivo orc\n",
        "spark.read.orc('zoo.orc').show()"
      ],
      "metadata": {
        "id": "vhpMBx5qqXt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trabalhando com SQL¶"
      ],
      "metadata": {
        "id": "dxGbs0Syqix6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#chamando meu dataframe e trandormando-o em uma tabela sql\n",
        "df.createOrReplaceTempView(\"tableA\")\n",
        "\n",
        "#fazendo a leitura dessa tabela sql\n",
        "spark.sql(\"SELECT count(*) from tableA\").show()"
      ],
      "metadata": {
        "id": "lzRpteODqkRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Além disso, os UDFs podem ser registrados e invocados no SQL imediatamente:\n",
        "@pandas_udf(\"integer\")\n",
        "def add_one(s: pd.Series) -> pd.Series:\n",
        "    return s + 1\n",
        "\n",
        "spark.udf.register(\"add_one\", add_one)\n",
        "spark.sql(\"SELECT add_one(v1) FROM tableA\").show()"
      ],
      "metadata": {
        "id": "XP3-J19Vq4gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Essas expressões SQL podem ser misturadas diretamente e usadas como colunas PySpark.\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "df.selectExpr('add_one(v1)').show()\n",
        "df.select(expr('count(*)') > 0).show()"
      ],
      "metadata": {
        "id": "K-XVd1aYsP8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}